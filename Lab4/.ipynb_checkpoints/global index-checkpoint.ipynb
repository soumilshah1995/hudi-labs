{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794fc562",
   "metadata": {},
   "source": [
    "## Soumil Nitin Shah \n",
    "Bachelor in Electronic Engineering |\n",
    "Masters in Electrical Engineering | \n",
    "Master in Computer Engineering |\n",
    "\n",
    "* Website : http://soumilshah.com/\n",
    "* Github: https://github.com/soumilshah1995\n",
    "* Linkedin: https://www.linkedin.com/in/shah-soumil/\n",
    "* Blog: https://soumilshah1995.blogspot.com/\n",
    "* Youtube : https://www.youtube.com/channel/UC_eOodxvwS_H7x2uLQa-svw?view_as=subscriber\n",
    "* Facebook Page : https://www.facebook.com/soumilshah1995/\n",
    "* Email : shahsoumil519@gmail.com\n",
    "* projects : https://soumilshah.herokuapp.com/project\n",
    "\n",
    "* I earned a Bachelor of Science in Electronic Engineering and a double masterâ€™s in Electrical and Computer Engineering. I have extensive expertise in developing scalable and high-performance software applications in Python. I have a YouTube channel where I teach people about Data Science, Machine learning, Elastic search, and AWS. I work as data Team Lead at Jobtarget where I spent most of my time developing Ingestion Framework and creating microservices and scalable architecture on AWS. I have worked with a massive amount of data which includes creating data lakes (1.2T) optimizing data lakes query by creating a partition and using the right file format and compression. I have also developed and worked on a streaming application for ingesting real-time streams data via kinesis and firehose to elastic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b39f51",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "###### Goal of this labs is to educate and teach you fundemental concepts on HUDI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a38100",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "##### Define Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba251f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    from pyspark.sql.functions import col, asc, desc\n",
    "    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    from datetime import datetime\n",
    "    from functools import reduce\n",
    "    from faker import Faker\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054bb22",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "#### Create Spark Instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6de397",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.12.1 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('className', 'org.apache.hudi') \\\n",
    "    .config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cbf0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JTSTDiSSHAH:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20fc28733d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd7439",
   "metadata": {},
   "source": [
    "# Step 3: \n",
    "#### Definje Hudi Settings for this project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab498bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"hudidb\"\n",
    "table_name = \"hudi_table\"\n",
    "\n",
    "recordkey = 'uuid'\n",
    "precombine = 'precomb'\n",
    "\n",
    "path = f\"file:///C:/tmp/{db_name}/{table_name}\"\n",
    "\n",
    "method = 'upsert'\n",
    "table_type = \"COPY_ON_WRITE\"  # COPY_ON_WRITE | MERGE_ON_READ\n",
    "partiton_field = \"partition\"\n",
    "\n",
    "\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': table_name,\n",
    "    'hoodie.datasource.write.recordkey.field': recordkey,\n",
    "    'hoodie.datasource.write.table.name': table_name,\n",
    "    'hoodie.datasource.write.operation': method,\n",
    "    'hoodie.datasource.write.precombine.field': precombine,\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.insert.shuffle.parallelism': 2,\n",
    "    'hoodie.datasource.write.partitionpath.field': partiton_field,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6bc94",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "#### Lets create out Hudidatalake and insert records and learn about precomb key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07d9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items = [\n",
    "    (1, \"This is APPEND 1\",  111, 1),\n",
    "    (2, \"This is APPEND 2\",  222, 2),\n",
    "]\n",
    "\n",
    "columns = [\"uuid\", \"message\", \"precomb\", \"partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da29f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(data=data_items, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4484abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------+---------+\n",
      "|uuid|         message|precomb|partition|\n",
      "+----+----------------+-------+---------+\n",
      "|   1|This is APPEND 1|    111|        1|\n",
      "|   2|This is APPEND 2|    222|        2|\n",
      "+----+----------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc725f26",
   "metadata": {},
   "source": [
    "# Step 5:\n",
    "##### Write the data into HUDI Table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fa99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"append\"). \\\n",
    "    save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "072a709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "      read. \\\n",
    "      format(\"hudi\"). \\\n",
    "      load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ba10bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------+-------------------+---------------------+\n",
      "|uuid|precomb|message         |_hoodie_commit_time|_hoodie_commit_seqno |\n",
      "+----+-------+----------------+-------------------+---------------------+\n",
      "|1   |111    |This is APPEND 1|20230108134654761  |20230108134654761_0_0|\n",
      "|2   |222    |This is APPEND 2|20230108134654761  |20230108134654761_0_1|\n",
      "+----+-------+----------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"uuid\", \"precomb\", \"message\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e8e5b",
   "metadata": {},
   "source": [
    "# Step 6:\n",
    "#### Understand the concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f612a",
   "metadata": {},
   "source": [
    "# Case 1: \n",
    "#### Same Precomb and ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f95fb6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------+-------------------+---------------------+\n",
      "|uuid|precomb|message         |_hoodie_commit_time|_hoodie_commit_seqno |\n",
      "+----+-------+----------------+-------------------+---------------------+\n",
      "|1   |111    |This is UPDATE 2|20230108134802216  |20230108134802216_0_0|\n",
      "|2   |222    |This is APPEND 2|20230108134654761  |20230108134654761_0_1|\n",
      "+----+-------+----------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_items = [\n",
    "    (1, \"This is UPDATE 1\",  111),\n",
    "    (1, \"This is UPDATE 2\",  111),\n",
    "]\n",
    "\n",
    "columns = [\"uuid\", \"message\", \"precomb\"]\n",
    "spark_df = spark.createDataFrame(data=data_items, schema=columns)\n",
    "\n",
    "spark_df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"append\"). \\\n",
    "    save(path)\n",
    "\n",
    "df = spark. \\\n",
    "      read. \\\n",
    "      format(\"hudi\"). \\\n",
    "      load(path)\n",
    "\n",
    "df.select([\"uuid\", \"precomb\", \"message\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\"]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c706322",
   "metadata": {},
   "source": [
    "# Case 2: \n",
    "### Same ID different precomb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "155cd193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------------------+-------------------+---------------------+\n",
      "|uuid|precomb|message            |_hoodie_commit_time|_hoodie_commit_seqno |\n",
      "+----+-------+-------------------+-------------------+---------------------+\n",
      "|1   |112    |This is UPDATE 2 **|20230108134924305  |20230108134924305_0_0|\n",
      "|2   |222    |This is APPEND 2   |20230108134654761  |20230108134654761_0_1|\n",
      "+----+-------+-------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_items = [\n",
    "    (1, \"This is UPDATE 1 ** \",  111),\n",
    "    (1, \"This is UPDATE 2 **\",  112),\n",
    "]\n",
    "\n",
    "columns = [\"uuid\", \"message\", \"precomb\"]\n",
    "spark_df = spark.createDataFrame(data=data_items, schema=columns)\n",
    "\n",
    "spark_df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"append\"). \\\n",
    "    save(path)\n",
    "\n",
    "df = spark. \\\n",
    "      read. \\\n",
    "      format(\"hudi\"). \\\n",
    "      load(path)\n",
    "\n",
    "df.select([\"uuid\", \"precomb\", \"message\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\"]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22195437",
   "metadata": {},
   "source": [
    "# Case 3: \n",
    "### Same ID different precomb switching Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57421943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+-------------------+---------------------+\n",
      "|uuid|precomb|message             |_hoodie_commit_time|_hoodie_commit_seqno |\n",
      "+----+-------+--------------------+-------------------+---------------------+\n",
      "|1   |115    |This is UPDATE 2 ## |20230108135029978  |20230108135029978_0_0|\n",
      "|2   |222    |This is APPEND 2    |20230108134654761  |20230108134654761_0_1|\n",
      "+----+-------+--------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_items = [\n",
    "    (1, \"This is UPDATE 1 ## \",  114),\n",
    "    (1, \"This is UPDATE 2 ## \",  115),\n",
    "]\n",
    "\n",
    "columns = [\"uuid\", \"message\", \"precomb\"]\n",
    "spark_df = spark.createDataFrame(data=data_items, schema=columns)\n",
    "\n",
    "spark_df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"append\"). \\\n",
    "    save(path)\n",
    "\n",
    "df = spark. \\\n",
    "      read. \\\n",
    "      format(\"hudi\"). \\\n",
    "      load(path)\n",
    "\n",
    "df.select([\"uuid\", \"precomb\", \"message\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\"]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "368799f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------------------+-------------------+---------------------+\n",
      "|uuid|precomb|message            |_hoodie_commit_time|_hoodie_commit_seqno |\n",
      "+----+-------+-------------------+-------------------+---------------------+\n",
      "|1   |115    |This is UPDATE 1 @ |20230108135121430  |20230108135121430_0_0|\n",
      "|2   |222    |This is APPEND 2   |20230108134654761  |20230108134654761_0_1|\n",
      "+----+-------+-------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_items = [\n",
    "    (1, \"This is UPDATE 1 @ \",  115),\n",
    "    (1, \"This is UPDATE 2 @ \",  112),\n",
    "]\n",
    "\n",
    "columns = [\"uuid\", \"message\", \"precomb\"]\n",
    "spark_df = spark.createDataFrame(data=data_items, schema=columns)\n",
    "\n",
    "spark_df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"append\"). \\\n",
    "    save(path)\n",
    "\n",
    "df = spark. \\\n",
    "      read. \\\n",
    "      format(\"hudi\"). \\\n",
    "      load(path)\n",
    "\n",
    "df.select([\"uuid\", \"precomb\", \"message\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\"]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a6a4d",
   "metadata": {},
   "source": [
    "### Conculsion \n",
    "* When you have same ID and Same PRECOMB Key HUDI will take the Most recent Items.\n",
    "* When you have same ID  but if oprecomb key is different HUDI will take items with new precomb key "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
